import os
from google.cloud import storage

def list_dynamic_gcs_folder_structure():
    # 1ï¸âƒ£ Detect current script path
    local_path = os.path.abspath(__file__)
    print(f"ğŸ“„ Local path: {local_path}")

    # 2ï¸âƒ£ Try to get Composer bucket dynamically
    bucket_name = os.getenv("GCS_BUCKET")

    # 3ï¸âƒ£ If not running in Composer, handle gracefully
    if not bucket_name:
        print("âš ï¸ Environment variable 'GCS_BUCKET' not found.")
        print("â„¹ï¸ Not running in Cloud Composer â€” please set bucket_name manually if needed.")
        return {
            "bucket": None,
            "prefix": None,
            "files": [],
            "folders": []
        }

    # 4ï¸âƒ£ Convert local path to GCS prefix
    # Example: /home/airflow/gcs/dags/...  â†’  dags/...
    gcs_prefix = local_path.replace("/home/airflow/gcs/", "")
    gcs_prefix = os.path.dirname(gcs_prefix)
    if not gcs_prefix.endswith("/"):
        gcs_prefix += "/"

    print(f"ğŸª£ Detected bucket: {bucket_name}")
    print(f"ğŸ“ Detected prefix: {gcs_prefix}")

    # 5ï¸âƒ£ Initialize GCS client
    client = storage.Client()

    # 6ï¸âƒ£ Use delimiter='/' to get folder-like hierarchy
    iterator = client.list_blobs(bucket_name, prefix=gcs_prefix, delimiter='/')

    files = []
    for blob in iterator:
        files.append(blob.name)

    folders = list(iterator.prefixes)

    # 7ï¸âƒ£ Print results
    print("\nğŸ“‚ Files directly under:", gcs_prefix)
    for f in files:
        print("   ğŸ—", f)

    print("\nğŸ“ Subfolders under:", gcs_prefix)
    for sf in folders:
        print("   ğŸ“‚", sf)

    return {
        "bucket": bucket_name,
        "prefix": gcs_prefix,
        "files": files,
        "folders": folders
    }

# Run directly
if __name__ == "__main__":
    list_dynamic_gcs_folder_structure()
