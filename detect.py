import os
from google.cloud import storage

def list_dynamic_gcs_folder_structure():
    # 1ï¸âƒ£ Detect current script path
    local_path = os.path.abspath(__file__)
    print(f"Local path: {local_path}")

    # 2ï¸âƒ£ Get Composer bucket name dynamically
    # Cloud Composer automatically sets this env variable
    bucket_name = os.getenv("GCS_BUCKET")

    if not bucket_name:
        raise EnvironmentError("GCS_BUCKET not found. Not running in Composer?")

    # 3ï¸âƒ£ Convert local DAG path to GCS prefix
    # /home/airflow/gcs/dags/...  â†’  dags/...
    gcs_prefix = local_path.replace("/home/airflow/gcs/", "")
    # remove file name to get folder only
    gcs_prefix = os.path.dirname(gcs_prefix)
    if not gcs_prefix.endswith("/"):
        gcs_prefix += "/"

    print(f"Detected bucket: {bucket_name}")
    print(f"Detected prefix: {gcs_prefix}")

    # 4ï¸âƒ£ Initialize GCS client
    client = storage.Client()

    # 5ï¸âƒ£ Use delimiter='/' to get folder-like hierarchy
    iterator = client.list_blobs(bucket_name, prefix=gcs_prefix, delimiter='/')

    print("\nğŸ“‚ Files directly under:", gcs_prefix)
    for blob in iterator:
        print("  ğŸ—", blob.name)

    print("\nğŸ“ Subfolders under:", gcs_prefix)
    for sub_prefix in iterator.prefixes:
        print("  ğŸ“‚", sub_prefix)

    return {
        "bucket": bucket_name,
        "prefix": gcs_prefix,
        "files": [b.name for b in iterator],
        "folders": list(iterator.prefixes)
    }

# Call this inside Composer
if __name__ == "__main__":
    list_dynamic_gcs_folder_structure()
